{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib2 import Path\n",
    "from tqdm import trange\n",
    "\n",
    "from dataloader import load_data_100k\n",
    "from model import GLocalNet, KernelNet\n",
    "from metric import ndcg, rmse_matrix, mae_matrix\n",
    "from loss import glocal_loss\n",
    "from utils import set_all_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./movie_lens_100k/')\n",
    "\n",
    "ckpt_dir = Path('./checkpoints/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "set_all_random_seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "n_hid = 500 # size of hidden layers\n",
    "n_emb = 5 # AE embedding size\n",
    "n_layers = 2 # number of hidden layers\n",
    "gk_size = 3 # width=height of kernel for convolution\n",
    "\n",
    "# Training hyperparameters\n",
    "max_epoch_p = 500 # max number of epochs for pretraining\n",
    "max_epoch_f = 1000 # max number of epochs for finetuning\n",
    "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
    "patience_f = 10 # and finetuning\n",
    "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
    "tol_f = 1e-5 # and finetuning\n",
    "lambda_L2 = 20. # regularisation of number or parameters\n",
    "lambda_sparse = 0.006 # regularisation of sparsity of the final matrix\n",
    "dot_scale = 1 # dot product weight for global kernel\n",
    "lr_p = 1e-3 # learning rate for pretraining\n",
    "lr_ft = 5e-3 # learning rate for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt(data_dir / 'movielens_100k_u1.base',\n",
    "                       skiprows=0,\n",
    "                       delimiter='\\t').astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 72000\n",
      "num of validation ratings: 8000\n",
      "num of test ratings: 20000\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data, r indicate rating matrix, m indicate mask matrix (0 for missing 1 for existing)\n",
    "n_m, n_u, train_R, train_M, val_R, val_M, test_R, test_M = load_data_100k(data_dir)\n",
    "\n",
    "# Model\n",
    "kernel_net = KernelNet(n_u, n_hid, n_emb, n_layers, lambda_sparse, lambda_L2)\n",
    "kernel_net.to(device)\n",
    "complete_model = GLocalNet(kernel_net, n_m, gk_size, dot_scale)\n",
    "complete_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_p = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=lr_p)\n",
    "optimizer_ft = torch.optim.AdamW(complete_model.parameters(), lr=lr_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining (local features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_rmse = float('inf')\n",
    "count = 0\n",
    "\n",
    "X = torch.Tensor(train_R).to(device)\n",
    "M = torch.Tensor(train_M).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 41/500 [00:31<05:49,  1.31it/s, epoch=40, train_rmse=0.937, val_rmse=0.963]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m complete_model\u001b[39m.\u001b[39mlocal_kernel_net\u001b[39m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m     pred \u001b[39m=\u001b[39m complete_model\u001b[39m.\u001b[39;49mlocal_kernel_net(X)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     15\u001b[0m pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(pred, \u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[1;32m     17\u001b[0m val_rmse \u001b[39m=\u001b[39m rmse_matrix(pred, val_M, val_R)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/si671project/model.py:97\u001b[0m, in \u001b[0;36mKernelNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m---> 97\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/si671project/model.py:43\u001b[0m, in \u001b[0;36mKernelLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Local kernelised weight matrix\u001b[39;00m\n\u001b[1;32m     42\u001b[0m W_eff \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW \u001b[39m*\u001b[39m w_hat\n\u001b[0;32m---> 43\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(x, W_eff) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m     44\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(y)\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_sparse \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_l2 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[39m# Sparse regularisation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with trange(max_epoch_p) as t:\n",
    "    for epoch in t:\n",
    "        # Training\n",
    "        complete_model.local_kernel_net.train()\n",
    "        optimizer_p.zero_grad()\n",
    "        pred, reg_loss = complete_model.local_kernel_net(X)\n",
    "        loss = glocal_loss(pred, reg_loss, M, X)\n",
    "        loss.backward()\n",
    "        optimizer_p.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        complete_model.local_kernel_net.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = complete_model.local_kernel_net(X).cpu().numpy()\n",
    "        pred = np.clip(pred, 1, 5)\n",
    "            \n",
    "        val_rmse = rmse_matrix(pred, val_M, val_R)\n",
    "        train_rmse = rmse_matrix(pred, train_M, train_R)\n",
    "        t.set_postfix(epoch=epoch, train_rmse=train_rmse, val_rmse=val_rmse)\n",
    "        if last_rmse - train_rmse < tol_p:\n",
    "            last_rmse = train_rmse\n",
    "            count += 1\n",
    "        if count == patience_p:\n",
    "            print('Early stopping at epoch {} with train rmse {:.4f} and val rmse {:.4f}'.format(epoch, train_rmse, val_rmse))\n",
    "            break\n",
    "ckpt = {\n",
    "    'train_rmse': train_rmse,\n",
    "    'val_rmse': val_rmse,\n",
    "    'state_dict': complete_model.state_dict(),\n",
    "}\n",
    "torch.save(ckpt, str(ckpt_dir / 'pretrain.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning (global features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse, best_mae, best_ndcg, last_rmse = float(\"inf\"), float(\"inf\"), 0, float(\"inf\")\n",
    "best_epoch_rmse, best_epoch_mae, best_epoch_ndcg = 0, 0, 0\n",
    "\n",
    "X = torch.Tensor(train_R).to(device)\n",
    "M = torch.Tensor(train_M).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    complete_model.eval()\n",
    "    X_local = complete_model.local_kernel_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1000 [00:26<17:48,  1.10s/it, epoch=23, train_mae=0.791, train_ndcg=0.861, train_rmse=0.981, val_mae=0.799, val_ndcg=0.891, val_rmse=0.999]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m pred, reg_loss \u001b[39m=\u001b[39m complete_model(X, X_local)\n\u001b[1;32m      7\u001b[0m loss \u001b[39m=\u001b[39m glocal_loss(pred, reg_loss, M, X)\n\u001b[0;32m----> 8\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      9\u001b[0m optimizer_ft\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m \u001b[39m# Evaluation\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with trange(max_epoch_f) as t:\n",
    "    for epoch in t:\n",
    "        # Training\n",
    "        complete_model.train()\n",
    "        optimizer_ft.zero_grad()\n",
    "        pred, reg_loss = complete_model(X, X_local)\n",
    "        loss = glocal_loss(pred, reg_loss, M, X)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        # Evaluation\n",
    "        complete_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = complete_model(X, X_local).cpu().numpy()\n",
    "        pred = np.clip(pred, 1, 5)\n",
    "\n",
    "        train_rmse = rmse_matrix(pred, train_M, train_R)\n",
    "        train_mae = mae_matrix(pred, train_M, train_R)\n",
    "        train_ndcg = ndcg(pred, train_R)\n",
    "        val_rmse = rmse_matrix(pred, val_M, val_R)\n",
    "        val_mae = mae_matrix(pred, val_M, val_R)\n",
    "        val_ndcg = ndcg(pred, val_R)\n",
    "\n",
    "        t.set_postfix(epoch=epoch,\n",
    "                    train_mae=train_mae,\n",
    "                    val_mae=val_mae,\n",
    "                    train_rmse=train_rmse,\n",
    "                    val_rmse=val_rmse,\n",
    "                    train_ndcg=train_ndcg,\n",
    "                    val_ndcg=val_ndcg)\n",
    "\n",
    "        if val_mae < best_mae:\n",
    "            best_mae = val_mae\n",
    "            best_epoch_mae = epoch\n",
    "            ckpt = {\n",
    "                'mae': val_mae,\n",
    "                'rmse': val_rmse,\n",
    "                'ndcg': val_ndcg,\n",
    "                'state_dict': complete_model.state_dict()\n",
    "            }\n",
    "            torch.save(ckpt, str(ckpt_dir / 'finetune_best_mae.pth'))\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_epoch_rmse = epoch\n",
    "            ckpt = {\n",
    "                'mae': val_mae,\n",
    "                'rmse': val_rmse,\n",
    "                'ndcg': val_ndcg,\n",
    "                'state_dict': complete_model.state_dict()\n",
    "            }\n",
    "            torch.save(ckpt, str(ckpt_dir / 'finetune_best_rmse.pth'))\n",
    "        if val_ndcg > best_ndcg:\n",
    "            best_ndcg = val_ndcg\n",
    "            best_epoch_ndcg = epoch\n",
    "            ckpt = {\n",
    "                'mae': val_mae,\n",
    "                'rmse': val_rmse,\n",
    "                'ndcg': val_ndcg,\n",
    "                'state_dict': complete_model.state_dict()\n",
    "            }\n",
    "            torch.save(ckpt, str(ckpt_dir / 'finetune_best_ndcg.pth'))\n",
    "        \n",
    "        if last_rmse - val_rmse < tol_f:\n",
    "            last_rmse = val_rmse\n",
    "            count += 1\n",
    "        if count == patience_f:\n",
    "            print('Early stopping at epoch {} with train rmse {:.4f} and val rmse {:.4f}'.format(epoch, train_rmse, val_rmse))\n",
    "            break\n",
    "\n",
    "print('Epoch:', best_epoch_rmse, 'Best RMSE:', best_rmse)\n",
    "print('Epoch:', best_epoch_mae, 'Best MAE:', best_mae)\n",
    "print('Epoch:', best_epoch_ndcg, 'Best NDCG:', best_ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.0181373626351828\n",
      "Test MAE: 0.81879437\n",
      "Test NDCG: 0.8688609693485918\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "complete_model.load_state_dict(torch.load(str(ckpt_dir / 'finetune_best_rmse.pth'))['state_dict'])\n",
    "complete_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = complete_model(X, X_local).cpu().numpy()\n",
    "pred = np.clip(pred, 1, 5)\n",
    "test_rmse = rmse_matrix(pred, test_M, test_R)\n",
    "test_ndcg = ndcg(pred, test_R)\n",
    "test_mae = mae_matrix(pred, test_M, test_R)\n",
    "print('Test RMSE:', test_rmse)\n",
    "print('Test MAE:', test_mae)\n",
    "print('Test NDCG:', test_ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8096d1454e3624a326c6294fdb446e72542397b4160fa8318f9dc24bee15a75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
