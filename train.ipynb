{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib2 import Path\n",
    "from tqdm import trange\n",
    "\n",
    "from dataloader import load_data_100k\n",
    "from model import GLocalNet, KernelNet\n",
    "from metric import ndcg, rmse_matrix, mae_matrix\n",
    "from loss import glocal_loss\n",
    "from utils import set_all_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./movie_lens_100k/')\n",
    "\n",
    "ckpt_dir = Path('./checkpoints/exp1/')\n",
    "if not ckpt_dir.exists():\n",
    "    ckpt_dir.mkdir()\n",
    "\n",
    "weights_dir = Path('./weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "set_all_random_seed(42)\n",
    "\n",
    "# Model hyperparameters\n",
    "n_hid = 500 # size of hidden layers\n",
    "n_emb = 4 # AE embedding size\n",
    "n_layers = 2 # number of hidden layers\n",
    "gk_size = 3 # width=height of kernel for convolution\n",
    "\n",
    "# Training hyperparameters\n",
    "max_epoch_p = 500 # max number of epochs for pretraining\n",
    "max_epoch_f = 1000 # max number of epochs for finetuning\n",
    "patience_p = 10# number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
    "patience_f = 10 # and finetuning\n",
    "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
    "tol_f = 1e-6 # and finetuning\n",
    "lambda_L2 = 20. # regularisation of number or parameters\n",
    "lambda_sparse = 0.006 # regularisation of sparsity of the final matrix\n",
    "dot_scale = 1 # dot product weight for global kernel\n",
    "lr_p = 1e-3 # learning rate for pretraining\n",
    "lr_ft = 1e-3# learning rate for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 76000\n",
      "num of validation ratings: 4000\n",
      "num of test ratings: 20000\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data, r indicate rating matrix, m indicate mask matrix (0 for missing 1 for existing)\n",
    "n_m, n_u, train_R, train_M, val_R, val_M, test_R, test_M = load_data_100k(data_dir)\n",
    "\n",
    "# Model\n",
    "kernel_net = KernelNet(n_u, n_hid, n_emb, n_layers, lambda_sparse, lambda_L2)\n",
    "kernel_net.to(device)\n",
    "complete_model = GLocalNet(kernel_net, n_m, gk_size, dot_scale)\n",
    "complete_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_p = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=lr_p)\n",
    "optimizer_ft = torch.optim.AdamW(complete_model.parameters(), lr=lr_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining (local features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_rmse, best_val_rmse = float('inf'), float('inf')\n",
    "count = 0\n",
    "\n",
    "X = torch.Tensor(train_R).to(device)\n",
    "M = torch.Tensor(train_M).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 447/500 [00:10<00:01, 43.88it/s, epoch=447, train_rmse=0.919, val_rmse=0.939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 447 with train rmse 0.9188 and val rmse 0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with trange(max_epoch_p) as t:\n",
    "    for epoch in t:\n",
    "        # Training\n",
    "        complete_model.local_kernel_net.train()\n",
    "        optimizer_p.zero_grad()\n",
    "        pred, reg_loss = complete_model.local_kernel_net(X)\n",
    "        loss = glocal_loss(pred, reg_loss, M, X)\n",
    "        loss.backward()\n",
    "        optimizer_p.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        complete_model.local_kernel_net.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = complete_model.local_kernel_net(X).cpu().numpy()\n",
    "        pred = np.clip(pred, 1, 5)\n",
    "            \n",
    "        val_rmse = rmse_matrix(pred, val_M, val_R)\n",
    "        train_rmse = rmse_matrix(pred, train_M, train_R)\n",
    "        t.set_postfix(epoch=epoch, train_rmse=train_rmse, val_rmse=val_rmse)\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            ckpt = {\n",
    "            'train_rmse': train_rmse,\n",
    "            'val_rmse': val_rmse,\n",
    "            'state_dict': complete_model.state_dict()}\n",
    "            torch.save(ckpt, str(ckpt_dir / 'pretrain.pth'))\n",
    "        if abs(last_rmse - train_rmse) < tol_p:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        if count == patience_p:\n",
    "            print('Early stopping at epoch {} with train rmse {:.4f} and val rmse {:.4f}'.format(epoch, train_rmse, val_rmse))\n",
    "            break\n",
    "        last_rmse = train_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning (global features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse, best_mae, best_ndcg, last_rmse = float(\"inf\"), float(\"inf\"), 0, float(\"inf\")\n",
    "best_epoch_rmse, best_epoch_mae, best_epoch_ndcg = 0, 0, 0\n",
    "\n",
    "complete_model.load_state_dict(torch.load(str(ckpt_dir / 'pretrain.pth'))['state_dict'])\n",
    "\n",
    "X = torch.Tensor(train_R).to(device)\n",
    "M = torch.Tensor(train_M).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    complete_model.eval()\n",
    "    X_local = complete_model.local_kernel_net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s, epoch=999, train_mae=0.665, train_ndcg=0.908, train_rmse=0.843, val_mae=0.711, val_ndcg=0.927, val_rmse=0.915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244 Best RMSE: 0.912213141932683\n",
      "Epoch: 986 Best MAE: 0.710681\n",
      "Epoch: 992 Best NDCG: 0.929717053698112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with trange(max_epoch_f) as t:\n",
    "    for epoch in t:\n",
    "        # Training\n",
    "        complete_model.train()\n",
    "        optimizer_ft.zero_grad()\n",
    "        pred, reg_loss = complete_model(X, X_local)\n",
    "        loss = glocal_loss(pred, reg_loss, M, X)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        # Evaluation\n",
    "        complete_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = complete_model(X, X_local).cpu().numpy()\n",
    "        pred = np.clip(pred, 1, 5)\n",
    "\n",
    "        train_rmse = rmse_matrix(pred, train_M, train_R)\n",
    "        train_mae = mae_matrix(pred, train_M, train_R)\n",
    "        train_ndcg = ndcg(pred, train_R)\n",
    "        val_rmse = rmse_matrix(pred, val_M, val_R)\n",
    "        val_mae = mae_matrix(pred, val_M, val_R)\n",
    "        val_ndcg = ndcg(pred, val_R)\n",
    "\n",
    "        t.set_postfix(epoch=epoch,\n",
    "                    train_mae=train_mae,\n",
    "                    val_mae=val_mae,\n",
    "                    train_rmse=train_rmse,\n",
    "                    val_rmse=val_rmse,\n",
    "                    train_ndcg=train_ndcg,\n",
    "                    val_ndcg=val_ndcg)\n",
    "\n",
    "        if val_mae < best_mae:\n",
    "            best_mae = val_mae\n",
    "            best_epoch_mae = epoch\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_epoch_rmse = epoch\n",
    "            ckpt = {\n",
    "                'mae': val_mae,\n",
    "                'rmse': val_rmse,\n",
    "                'ndcg': val_ndcg,\n",
    "                'state_dict': complete_model.state_dict()\n",
    "            }\n",
    "            torch.save(ckpt, str(ckpt_dir / 'finetune_best_rmse.pth'))\n",
    "        if val_ndcg > best_ndcg:\n",
    "            best_ndcg = val_ndcg\n",
    "            best_epoch_ndcg = epoch\n",
    "        \n",
    "        if abs(last_rmse - val_rmse) < tol_f:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "        if count == patience_f:\n",
    "            print('Early stopping at epoch {} with train rmse {:.4f} and val rmse {:.4f}'.format(epoch, train_rmse, val_rmse))\n",
    "            break\n",
    "        last_rmse = train_rmse\n",
    "\n",
    "print('Epoch:', best_epoch_rmse, 'Best RMSE:', best_rmse)\n",
    "print('Epoch:', best_epoch_mae, 'Best MAE:', best_mae)\n",
    "print('Epoch:', best_epoch_ndcg, 'Best NDCG:', best_ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9215279912065995\n",
      "Test MAE: 0.7257653\n",
      "Test NDCG: 0.8950813935668877\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "complete_model.load_state_dict(torch.load(str(ckpt_dir / 'finetune_best_rmse.pth'))['state_dict'])\n",
    "complete_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = complete_model(X, X_local).cpu().numpy()\n",
    "pred = np.clip(pred, 1, 5)\n",
    "test_rmse = rmse_matrix(pred, test_M, test_R)\n",
    "test_ndcg = ndcg(pred, test_R)\n",
    "test_mae = mae_matrix(pred, test_M, test_R)\n",
    "print('Test RMSE:', test_rmse)\n",
    "print('Test MAE:', test_mae)\n",
    "print('Test NDCG:', test_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model.cpu()\n",
    "torch.save(complete_model.state_dict(), str(weights_dir / 'best.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a7ad60629c7992fdadfcbec1ee9d18d5ed6e222b8ad46df459875a57fae42b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
